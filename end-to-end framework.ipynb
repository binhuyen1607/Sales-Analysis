{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>1. Look at the big Picture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Frame the Problems**\n",
    "\n",
    "* **What's the business objective**\n",
    "* **How does the company expect to use and benefit from the model?**\n",
    "* Frame problems --> chose algorithm --> performance measure --> ways/time to tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Chose performance measure (Loss)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.1 Regression**\n",
    "* RMSE\n",
    "* MAE\n",
    "* MSE, \n",
    "* RMSLE\n",
    "* MAPE\n",
    "* SMAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.2 Classification**\n",
    "* Binary Cross Entropy \n",
    "* Sparse Categorical Cross Entropy\n",
    "* Categorial Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Chose performance measure (Metrics)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3.1 Regression**\n",
    "* Adjusted r^2\n",
    "\n",
    "### **1.3.2 Classification**\n",
    "* AUC\n",
    "* Accuracy\n",
    "* False Positive\n",
    "* False Negative\n",
    "* F1 Score\n",
    "* Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Check the Assumptions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>2. Get the Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamin\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "#-----------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"file_name.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>3. Data Cleaning (General)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Formating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.1 Format column names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "   # convert form camel case to snake case\n",
    "   df.columns = (df.columns\n",
    "                  .str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True)\n",
    "                  .str.lower()\n",
    "               )\n",
    "   # convert \" \" to _\n",
    "   df.columns = df.columns.str.replace(' ', '_')\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rename_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename function:\n",
    "# df.rename(columns={'unnamed:_0': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.2 Format data body**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the columns model to make, version, engine, body type, trim, and doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version(df, column_name):\n",
    "    # Split the 'model' column by spaces\n",
    "    split_model = df[column_name].str.split(' ', expand=True)\n",
    "    \n",
    "    # Extract the part between the first and second space as the 'version'\n",
    "    version = split_model[1].where(split_model[1].str.match(r'^\\d+(\\.\\d+)?$'))\n",
    "    \n",
    "    # Assign the 'version' to the DataFrame\n",
    "    df['version'] = version\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_version(df, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.version.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doors(df, column_name):\n",
    "    # Extract the 'doors' component using regular expression\n",
    "    df['doors'] = df[column_name].str.extract(r'(\\d/\\d-Doors)')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_doors(df, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.doors.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more way to clean this df and extract this model (['make', 'version', 'engine', 'body_type', 'trim', 'doors']), but for the sake of time let move to other formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.3 Format Datatypes**\n",
    "After we format all the data body and columns, let's format the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datatypes are formatted correctly, so no need to further format datatypes, but in other scenario, it might be best format datatypes when there's date/datetime columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Drop duplicates rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>4. Attribute Combination</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['km_per_year'] = df['km']/df['age_08_04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 5. EDA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda col: col.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attribs = ['model', 'fuel_type', 'color', 'doors', 'version']\n",
    "bin_attribs = [col for col in df.columns if len(df[col].unique()) == 2 or len(df[col].unique()) == 1]\n",
    "num_attribs = [col for col in df.columns if col not in cat_attribs and col not in bin_attribs and col != 'id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Univariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns for subplots\n",
    "num_rows = 5\n",
    "num_cols = (len(num_attribs) + num_rows - 1) // num_rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array to iterate over numerical attributes\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over numerical attributes and plot histograms\n",
    "for i, column in enumerate(num_attribs):\n",
    "    sns.histplot(df[column], ax=axes[i], kde=True)\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i+1, num_rows*num_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows and columns for subplots\n",
    "num_rows = 5\n",
    "num_cols = (len(bin_attribs) + num_rows - 1) // num_rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 10))\n",
    "\n",
    "# Flatten the axes array to iterate over binary attributes\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over binary attributes and plot bar graphs\n",
    "for i, column in enumerate(bin_attribs):\n",
    "    sns.countplot(x=df[column], ax=axes[i])\n",
    "    axes[i].set_title(column)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(i+1, num_rows*num_cols):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each model\n",
    "model_counts = df['model'].value_counts().head(5)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 5 Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the occurrences of each fuel type\n",
    "fuel_counts = df['fuel_type'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "fuel_counts.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Fuel Type Distribution')\n",
    "plt.xlabel('Fuel Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each color\n",
    "color_counts = df['color'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "color_counts.plot(kind='bar', color='salmon')\n",
    "plt.title('Color Distribution')\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 3\n",
    "num_cols = len(num_attribs) // num_rows + (len(num_attribs) % num_rows > 0)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8 * num_cols, 6 * num_rows))\n",
    "axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "for i, col in enumerate(num_attribs):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x=df[col], ax=ax, color='skyblue')  # Use seaborn's boxplot function\n",
    "    ax.set_title(f'Box plot for {col}')\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Bivariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[num_attribs])\n",
    "plt.title('Pairplot of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_curves(df, columns, hue_column):\n",
    "    \"\"\"\n",
    "    Plot distribution curves for each column in the DataFrame\n",
    "    with respect to the specified hue column using different colors.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing the data.\n",
    "        columns (list): List of columns to plot.\n",
    "        hue_column (str): Column to use for coloring the distribution curves.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    colors = sns.color_palette(\"husl\", len(df[hue_column].unique()))\n",
    "    \n",
    "    num_plots = len(columns)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=num_plots // 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes to 1D for easy iteration\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        for j, value in enumerate(df[hue_column].unique()):\n",
    "            sns.kdeplot(data=df[df[hue_column] == value], x=col, color=colors[j], ax=axes[i])\n",
    "        axes[i].set_title(f\"Distribution of {col} by {hue_column}\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].legend(df[hue_column].unique(), title=hue_column)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_curves(df, num_attribs, 'fuel_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 Multivariate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[num_attribs].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 6. Labels Cleaning (Optionals)</h1>\n",
    "\n",
    "* Drop null labels\n",
    "* Drop outlier labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 7. Split Train Test For Modelling </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs.remove(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "train = train_set.drop(target, axis=1)\n",
    "train_labels = train_set[[target]]\n",
    "\n",
    "train_num = train[num_attribs+bin_attribs]\n",
    "train_cat = train[cat_attribs]\n",
    "\n",
    "\n",
    "test = test_set.drop(target, axis=1)\n",
    "test_labels = test_set[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 8. Data Cleaning for Modelling </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "null_counts = train_set.isnull().sum()\n",
    "\n",
    "# Filter columns with more than zero null values\n",
    "null_counts_gt_zero = null_counts[null_counts > 0]\n",
    "\n",
    "# Plotting the null counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "null_counts_gt_zero.plot(kind='bar', color='skyblue')\n",
    "plt.title('Count of Null Values in DataFrame Columns (Columns with >0 Nulls)')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Count of Null Values')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.1 Handle Nulls**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Types of NULLS\n",
    "\n",
    "**Missing Completely at Random (MCAR)**\n",
    "- Missing data is random\n",
    "- Data was lost in ETL, someone was interrupted when completing a survey\n",
    "- Remove or impute\n",
    "\n",
    "**Missing at Random (MAR)**\n",
    "- Missing data suggests something about something else observed\n",
    "- Older (which we have in data) may have higher privacy concerns and not report income\n",
    "- Remove or impute\n",
    "\n",
    "**Missing Not at Random (MNAR)**\n",
    "- Missing based on something not observed\n",
    "- Self-selection bias: Depressed do not complete mental health surveys\n",
    "- Advanced econometrics\n",
    "\n",
    "**By Design:**\n",
    "- Remove credit card data if birthdate is below a certain value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Solutions to handle Nulls \n",
    "- Get rid of the corresponding rows.\n",
    "- Get rid of the whole attribute.\n",
    "- Set the values to some value (zero, the mean, the median, etc.).\n",
    "- Create a new column that have True for the non-null and False for null (or the opposite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=[\"minimum_payments\"]) # option 1\n",
    "# df.drop(\"minimum_payments\", axis=1) # option 2\n",
    "# median = df[\"minimum_payments\"].median() # option 3\n",
    "# df[\"minimum_payments\"].fillna(median, inplace=True) # option 4\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputer = SimpleImputer(strategy=\"median\")\n",
    "train_labels_imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit train (non_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputer.fit(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_imputer.transform(train_num) # return Numpy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num =  pd.DataFrame(X, columns=train_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_imputer.fit(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_imputer.statistics_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_labels_imputer.transform(train_labels) # return Numpy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels =  pd.DataFrame(Y, columns=train_labels.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.2 Handle Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 3\n",
    "num_cols = len(num_attribs) // num_rows + (len(num_attribs) % num_rows > 0)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8 * num_cols, 6 * num_rows))\n",
    "axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "for i, col in enumerate(num_attribs):\n",
    "    ax = axes[i]\n",
    "    sns.boxplot(x=df[col], ax=ax, color='skyblue')  # Use seaborn's boxplot function\n",
    "    ax.set_title(f'Box plot for {col}')\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outlier\n",
    "def remove_outliers(df, column_names=None):\n",
    "    \"\"\"\n",
    "    Remove outliers from specific columns in the DataFrame based on the interquartile range (IQR) method,\n",
    "    or remove outliers from all numerical columns if column_names is None.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "        The DataFrame containing the data.\n",
    "    - column_names: list or None, default None\n",
    "        The list of column names for which outliers are to be removed,\n",
    "        or None to remove outliers from all numerical columns.\n",
    "\n",
    "    Returns:\n",
    "    - df_filtered: DataFrame\n",
    "        The DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    if column_names is None:\n",
    "        num_attribs = df.select_dtypes(include='number').columns\n",
    "    else:\n",
    "        num_attribs = column_names\n",
    "\n",
    "    total_removed = 0\n",
    "    total_rows = len(df)\n",
    "\n",
    "    for col in num_attribs:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define the lower and upper bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Remove outliers from the specified column\n",
    "        removed_rows = len(df) - len(df[(df[col] >= lower_bound) & (df[col] <= upper_bound)])\n",
    "        total_removed += removed_rows\n",
    "\n",
    "        # Update DataFrame\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "        # Print the number and percentage of removed values if any rows have been removed\n",
    "        percentage_removed = (removed_rows / total_rows) * 100\n",
    "        print(f\"Removed {removed_rows} rows ({percentage_removed:.2f}%) due to outliers in column '{col}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not handling any outlier in this case to see how the model perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 9.Preprocess Data for Modelling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9.1 Handle Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(train_cat[cat_attribs])\n",
    "housing_cat_encoded[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder.categories_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(train_cat[cat_attribs])\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9.2 Feature Scaling**\n",
    "* As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 Min-Max Scailing (Normalization Scailing)\n",
    "* Values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min\n",
    "* Normalization are more affected by outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_dataframe(df, columns_to_scale=None):\n",
    "    \"\"\"\n",
    "    Scale the specified columns in the DataFrame using Min-Max scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame to be scaled.\n",
    "        columns_to_scale (list): List of columns to be scaled. If None, scale all numerical columns.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Scaled DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    if columns_to_scale is None:\n",
    "        columns_to_scale = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Standardization Scailing \n",
    "* First it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance\n",
    "*  Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1)\n",
    "* standardization is much less affected by outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale_dataframe(df, columns_to_scale=None):\n",
    "    \"\"\"\n",
    "    Scale the specified columns in the DataFrame using standard scaling.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame to be scaled.\n",
    "        columns_to_scale (list): List of columns to be scaled. If None, scale all numerical columns.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Scaled DataFrame.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    if columns_to_scale is None:\n",
    "        columns_to_scale = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9.3 Transformation Pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    " ('imputer', SimpleImputer(strategy=\"median\")),\n",
    " ('std_scaler', StandardScaler()),\n",
    " ])\n",
    "# housing_num_tr = num_pipeline.fit_transform(train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    " (\"num\", num_pipeline, num_attribs+bin_attribs),\n",
    " (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_attribs),\n",
    " ])\n",
    "train_prepared = full_pipeline.fit_transform(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_imputer.fit(train_labels.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_labels_imputer.transform(train_labels.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels =  pd.DataFrame(Y, columns=train_labels.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 10. Select and Train a Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10.1 Training and Evaluating on the Training Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.1 Model 1: Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = train.iloc[:5]\n",
    "some_labels = train_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", np.array(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(train_prepared)\n",
    "lin_mse = mean_squared_error(np.array(train_labels), housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = np.array(train_labels) - housing_predictions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Plot QQ plot\n",
    "sm.qqplot(residuals, line = '45')\n",
    "plt.title('QQ Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.2 Model 2: Decision Tree Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(train_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(train_prepared)\n",
    "tree_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = np.array(train_labels) - housing_predictions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Plot QQ plot\n",
    "sm.qqplot(residuals, line='45')\n",
    "plt.title('QQ Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 Model 3: Forest Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(train_prepared, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = forest_reg.predict(train_prepared)\n",
    "forest_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "print(forest_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = np.array(train_labels) - housing_predictions\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Plot QQ plot\n",
    "sm.qqplot(residuals, line='45')\n",
    "plt.title('QQ Plot of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potentially Saving the model for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the trained model to a file\n",
    "# joblib.dump(forest_reg, \"forest_reg.pkl\")\n",
    "\n",
    "# # Later, when you want to use the model again\n",
    "# # Load the model from the file\n",
    "# forest_reg = joblib.load(\"forest_reg.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could this model really be absolutely perfect? Of course,\n",
    "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
    "As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
    "model you are confident about, so you need to use part of the training set for train‐\n",
    "ing, and part for model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10.2 Better Evaluation Using Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(tree_reg, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, train_prepared, train_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_scores = cross_val_score(forest_reg, train_prepared, train_labels,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. The solution include:\n",
    "* Simplify the model\n",
    "* Constrain it (i.e., regularize it)\n",
    "* Get a lot more training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try others ML models, the goal is to shortlist 2-5 promising models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong> 11. Fine Tune Your Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11.1 Grid Search**\n",
    "One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\n",
    "\n",
    "\n",
    "Instead you should get Scikit-Learn’s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    " {'n_estimators': [5, 25, 50], 'max_features': [5, 10, 15, 25]},\n",
    " {'bootstrap': [False], 'n_estimators': [5, 10], 'max_features': [5, 10, 15]},\n",
    " ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    " scoring='neg_mean_squared_error',\n",
    "return_train_score=True)\n",
    "grid_search.fit(train_prepared, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV(forest_reg, param_grid, cv=5,\n",
    " scoring='neg_mean_squared_error',\n",
    "return_train_score=True)\n",
    "grid_search.fit(train_prepared, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of n_estimators and max_features hyperparameter values specified in the first dict. It will then try all 2 × 3 = 6 combinations of hyperparameter values in the second dict, but this time with the bootstrap hyperparameter set to False instead of True (which is the default value for this hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non of the fine tune model are as good as the orininal one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11.2 Randomized Search**\n",
    "Better when the hyperparameter space is large. This approach is the same way as the GridSearchCV class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "* If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).\n",
    "* You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11.3 Ensemble Methods**\n",
    "Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>   12. Analyze Model and Explain Features important </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.1 Grid Search Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.2 Lime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.3 Shapley**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"><strong>   13 Evaluate Your System on the Test Set </h1>\n",
    "\n",
    "Now is the time to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set, run your full_pipeline to transform the data (call `transform`, not `fit_transform()`, you do not want to fit the test set!), and evaluate the final model on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = forest_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_set.drop(\"price\", axis=1)\n",
    "test_labels = test_set[\"price\"].copy()\n",
    "\n",
    "test_prepared = full_pipeline.transform(test)\n",
    "test_labels = train_labels_imputer.transform(test_labels.values.reshape(-1, 1))\n",
    "\n",
    "final_model.fit(test_prepared, test_labels)\n",
    "final_predictions = final_model .predict(test_prepared)\n",
    "final_mse = mean_squared_error(test_labels, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
